import logging
from abc import ABC, abstractmethod
from typing import Any, List

import numpy as np
import torch
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
import timm
from timm.data import resolve_model_data_config, create_transform

logger = logging.getLogger(__name__)


class Embedder(ABC):
    """Abstract base class for image embedders"""

    @abstractmethod
    async def embed_images(self, images: List[Any]) -> List[np.ndarray]:
        """
        Generate embeddings for a list of images

        Args:
            images: List of images (can be PIL Images, numpy arrays or file paths)

        Returns:
            List of embedding vectors
        """
        pass

    @property
    def name(self) -> str:
        """Return the name of this embedder"""
        pass

    @property
    def embedding_dim(self) -> int:
        """Return the dimension of embeddings generated by this embedder"""
        pass


class ClipEmbedder(Embedder):
    """CLIP-based image embedder"""

    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        """
        Initialize CLIP embedder

        Args:
            model_name: The CLIP model to use
        """
        self._name = "clip"
        try:
            self.clip_model = CLIPModel.from_pretrained(model_name)
            self.clip_processor = CLIPProcessor.from_pretrained(model_name)
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.clip_model.to(self.device)
            self._embedding_dim = self.clip_model.get_image_features(
                **self.clip_processor(images=[Image.new("RGB", (224, 224))], return_tensors="pt").to("cpu")).shape[1]
            logger.info(f"CLIP model loaded successfully. Using device: {self.device}")
        except Exception as e:
            logger.error(f"Error loading CLIP model: {str(e)}")
            self.clip_model = None
            self.clip_processor = None
            self._embedding_dim = 512  # Default CLIP dimension

    @property
    def name(self) -> str:
        return self._name

    @property
    def embedding_dim(self) -> int:
        return self._embedding_dim

    async def embed_images(self, images: List[Any]) -> List[np.ndarray]:
        """
        Generate embeddings for a list of images using CLIP

        Args:
            images: List of images (can be PIL Images, numpy arrays or file paths)

        Returns:
            List of embedding vectors
        """
        if not self.clip_model or not self.clip_processor:
            logger.error("CLIP model not initialized")
            return [np.zeros(self.embedding_dim) for _ in images]  # Return dummy embeddings

        embeddings = []
        batch_size = 16  # Process images in batches to avoid OOM

        for i in range(0, len(images), batch_size):
            batch = images[i:i + batch_size]
            processed_batch = []

            for img in batch:
                if isinstance(img, str):  # File path
                    try:
                        pil_img = Image.open(img).convert("RGB")
                        processed_batch.append(pil_img)
                    except Exception as e:
                        logger.error(f"Error loading image from {img}: {str(e)}")
                        processed_batch.append(Image.new("RGB", (224, 224)))
                elif isinstance(img, np.ndarray):  # Numpy array
                    try:
                        pil_img = Image.fromarray(img).convert("RGB")
                        processed_batch.append(pil_img)
                    except Exception as e:
                        logger.error(f"Error converting numpy array to PIL Image: {str(e)}")
                        processed_batch.append(Image.new("RGB", (224, 224)))
                else:  # Assume it's already a PIL Image or compatible
                    processed_batch.append(img)

            try:
                with torch.no_grad():
                    inputs = self.clip_processor(images=processed_batch, return_tensors="pt").to(self.device)
                    image_features = self.clip_model.get_image_features(**inputs)
                    # Normalize embeddings
                    image_features = image_features / image_features.norm(dim=1, keepdim=True)
                    # Convert to numpy and move to CPU
                    batch_embeddings = image_features.cpu().numpy()
                    embeddings.extend(batch_embeddings)
            except Exception as e:
                logger.error(f"Error generating embeddings: {str(e)}")
                # Add zero embeddings for the failed batch
                dummy_embedding = np.zeros(self.embedding_dim)
                embeddings.extend([dummy_embedding] * len(processed_batch))

        return embeddings


class TimmEmbedder(Embedder):
    """General embedder using timm models"""

    def __init__(self, model_name: str):
        """
        Initialize embedder using any timm model

        Args:
            model_name: Name of the model in timm library
        """
        self._name = model_name
        self._embedding_dim = 1000  # Default, will be updated when model is loaded
        try:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = timm.create_model(model_name, pretrained=True, num_classes=0).to(self.device)
            self.model.eval()

            # Get preprocessing transform
            self.preprocess = self.get_preprocess()

            # Update embedding dimension
            self._embedding_dim = self.model.num_features

            self.model.to(self.device)
            logger.info(f"Timm model '{model_name}' loaded successfully. Using device: {self.device}")
        except Exception as e:
            logger.error(f"Error loading timm model '{model_name}': {str(e)}")
            self.model = None
            self.preprocess = None

    def get_preprocess(self):
        # Unwrap the model if wrapped in DataParallel
        model_for_config = self.model.module if hasattr(self.model, 'module') else self.model
        data_config = resolve_model_data_config(model_for_config)
        return create_transform(**data_config, is_training=False)

    @property
    def name(self) -> str:
        return self._name

    @property
    def embedding_dim(self) -> int:
        return self._embedding_dim

    async def embed_images(self, images: List[Any]) -> List[np.ndarray]:
        """
        Generate embeddings for a list of images using the timm model

        Args:
            images: List of images (can be PIL Images, numpy arrays or file paths)

        Returns:
            List of embedding vectors
        """
        if not self.model or not self.preprocess:
            logger.error(f"Model {self.name} not initialized")
            return [np.zeros(self.embedding_dim) for _ in images]  # Return dummy embeddings

        embeddings = []
        batch_size = 16  # Process images in batches to avoid OOM

        for i in range(0, len(images), batch_size):
            batch = images[i:i + batch_size]
            processed_batch = []

            for img in batch:
                try:
                    if isinstance(img, str):
                        pil_img = Image.open(img).convert("RGB")
                    elif isinstance(img, np.ndarray):
                        pil_img = Image.fromarray(img).convert("RGB")
                    else:
                        pil_img = img

                    tensor = self.preprocess(pil_img).unsqueeze(0)
                    processed_batch.append(tensor)
                except Exception as e:
                    logger.error(f"Error processing image for {self.name}: {str(e)}")
                    processed_batch.append(torch.zeros(1, 3, 224, 224))

            try:
                with torch.no_grad():
                    batch_tensor = torch.cat(processed_batch, dim=0).to(self.device)
                    features = self.model(batch_tensor)
                    features = features / features.norm(dim=1, keepdim=True)
                    batch_embeddings = features.cpu().numpy()
                    embeddings.extend(batch_embeddings)
            except Exception as e:
                logger.error(f"Error generating {self.name} embeddings: {str(e)}")
                dummy_embedding = np.zeros(self.embedding_dim)
                embeddings.extend([dummy_embedding] * len(processed_batch))

        return embeddings